{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/amee342/semantic_role_labeling.git"
      ],
      "metadata": {
        "id": "3BFc60T4B9ln"
      },
      "id": "3BFc60T4B9ln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd semantic_role_labeling/"
      ],
      "metadata": {
        "id": "2v9jZ7zkCIho"
      },
      "id": "2v9jZ7zkCIho",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate evaluate seqeval"
      ],
      "metadata": {
        "id": "QF5k0MteCSGn"
      },
      "id": "QF5k0MteCSGn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up saving repo in drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ozH3cjJzCZOH"
      },
      "id": "ozH3cjJzCZOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Argument setting\n",
        "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "batch_size = 16\n",
        "task = \"SRL\"\n",
        "training_epoch = 1"
      ],
      "metadata": {
        "id": "77PKI4tWU0_W"
      },
      "id": "77PKI4tWU0_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed!\n",
        "\n",
        "SEED = 0\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "DS4VuAux0zk7"
      },
      "id": "DS4VuAux0zk7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is an repository in drive called \"SRL\" for storing finetuned models"
      ],
      "metadata": {
        "id": "Ppc39e9YCy7a"
      },
      "id": "Ppc39e9YCy7a"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, set_seed\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "SN4G5LdrQj1J"
      },
      "id": "SN4G5LdrQj1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Parse CONLLU dataset"
      ],
      "metadata": {
        "id": "5_xm9dk9DKC0"
      },
      "id": "5_xm9dk9DKC0"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conll_sentences(path: str):\n",
        "\n",
        "    sentences = []\n",
        "    sent = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "\n",
        "            line = line.rstrip(\"\\n\")\n",
        "\n",
        "            #  save previous sentence\n",
        "            # at boundary between 2 sentences\n",
        "            if line.strip() == \"\":\n",
        "                if sent:\n",
        "                    sentences.append(sent)\n",
        "                    sent = []\n",
        "                continue\n",
        "\n",
        "            # skip comments\n",
        "            if line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            cols = line.split(\"\\t\")\n",
        "            sent.append(cols)\n",
        "\n",
        "    if sent:\n",
        "        sentences.append(sent)\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "JZP6N7WtG8qW"
      },
      "id": "JZP6N7WtG8qW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "pIjvpOZ6QPVb"
      },
      "id": "pIjvpOZ6QPVb"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sentences_and_tokens(sentences: List):\n",
        "  \"\"\"\n",
        "  Return number of sentences (n_sent)\n",
        "  and number of tokens from these sentences (n_token)\n",
        "  \"\"\"\n",
        "  n_sent = len(sentences)\n",
        "  n_token = sum(len(s) for s in sentences)\n",
        "\n",
        "  return n_sent, n_token\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hwEk84QFCb1E"
      },
      "id": "hwEk84QFCb1E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate each sentence for each predicate"
      ],
      "metadata": {
        "id": "9miFvqIaQ7my"
      },
      "id": "9miFvqIaQ7my"
    },
    {
      "cell_type": "code",
      "source": [
        "def find_predicate_index(sent,\n",
        "                           label_col,\n",
        "                           predicate_markers=(\"V\", \"B-V\")):\n",
        "  for i, row in enumerate(sent):\n",
        "    if len(row) > label_col and row[label_col] in predicate_markers:\n",
        "      return i\n",
        "  return None\n",
        "\n"
      ],
      "metadata": {
        "id": "6P9kitpMRJKL"
      },
      "id": "6P9kitpMRJKL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replicate_sentences(sentences,\n",
        "                        base_cols: int=11):\n",
        "\n",
        "  instances = []\n",
        "  \"\"\"\n",
        "  base_cols: the column with specified predicates\n",
        "  \"\"\"\n",
        "  for sent in sentences:\n",
        "\n",
        "    # check the maximum columns in specific sentence\n",
        "    # assume it's consistent per token row\n",
        "    max_cols = max(len(r) for r in sent)\n",
        "\n",
        "    # nr of predicate-specific label columns\n",
        "    k = max(0, max_cols-base_cols)\n",
        "\n",
        "    if k == 0 :\n",
        "      # sentence has no predicate\n",
        "      continue\n",
        "\n",
        "    # rely on k\n",
        "    for j in range(k):\n",
        "      label_col = base_cols + j  # 0-based index\n",
        "\n",
        "      pred_index = find_predicate_index(sent, label_col)\n",
        "\n",
        "      # fallback if no V marker found\n",
        "      if pred_index is None:\n",
        "        pred_index = next((i for i,r in enumerate(sent) if len(r) > 9 and r[9] not in (\"_\", \"-\", \"\")), None)\n",
        "\n",
        "\n",
        "      tokens = [r[1] for r in sent] # FORM column\n",
        "\n",
        "      #labels = [(r[label_col] if len(r) > label_col else \"O\") for r in sent]\n",
        "      labels = [\n",
        "                    \"O\" if (len(r) <= label_col or r[label_col] == \"_\")\n",
        "                    else r[label_col]\n",
        "                    for r in sent\n",
        "              ]\n",
        "\n",
        "      instances.append({\n",
        "                \"tokens\": tokens,\n",
        "                \"predicate_index\": pred_index,\n",
        "                \"labels\": labels,\n",
        "            })\n",
        "  return instances\n",
        "\n"
      ],
      "metadata": {
        "id": "mHGFeEXGQrV4"
      },
      "id": "mHGFeEXGQrV4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(path:str):\n",
        "  sentences = load_conll_sentences(path)\n",
        "  before_s, before_t = count_sentences_and_tokens(sentences)\n",
        "\n",
        "  instances = replicate_sentences(sentences)\n",
        "  after_s, after_t = count_sentences_and_tokens(instances)\n",
        "\n",
        "  return {\n",
        "        \"sentences\": sentences,\n",
        "        \"instances\": instances,\n",
        "        \"stats\": {\n",
        "            \"before_sentences\": before_s,\n",
        "            \"before_tokens\": before_t,\n",
        "            \"after_instances\": after_s,\n",
        "            \"after_tokens\": after_t\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "PqKWAWqpRNSy"
      },
      "id": "PqKWAWqpRNSy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "mJFOF5ySUPsG"
      },
      "id": "mJFOF5ySUPsG"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "TLsfsxT0UPYV"
      },
      "id": "TLsfsxT0UPYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if tokenizer is backed by RUST\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "metadata": {
        "id": "e6GvXTBzV7EB"
      },
      "id": "e6GvXTBzV7EB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_all_tokens = True\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "  tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  word_ids = tokenized_inputs.word_ids()\n",
        "  previous_word_idx=None\n",
        "  label_ids=[]\n",
        "\n",
        "  for word_idx in word_ids:\n",
        "    if word_idx is None:\n",
        "      # for special token that is ignored in Pytorch,\n",
        "      # set as -100\n",
        "      label_ids.append(-100)\n",
        "    elif word_idx != previous_word_idx:\n",
        "      label_ids.append(example[\"labels\"][word_idx])\n",
        "    else:\n",
        "      label_ids.append(example[\"labels\"][word_idx]) if labels_all_tokens else label_ids.append(-100)\n",
        "\n",
        "    previous_word_idx = word_idx\n",
        "  return label_ids\n"
      ],
      "metadata": {
        "id": "ITkMjsYpbGQd"
      },
      "id": "ITkMjsYpbGQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_all_tokens = True  # or False\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "\n",
        "    for i, labels in enumerate(examples[\"labels_str\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[labels[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(label2id[labels[word_idx]] if labels_all_tokens else -100)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels  # <-- ints + -100\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "poDjZENQbGLQ"
      },
      "id": "poDjZENQbGLQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "\n",
        "dataset = load_and_preprocess(\"/content/semantic_role_labeling/data/en_ewt-up-test.conllu\")\n",
        "ds = Dataset.from_list(dataset['instances'])\n",
        "ds = ds.rename_column(\"labels\", \"labels_str\")\n"
      ],
      "metadata": {
        "id": "mM2b4JLxX_98"
      },
      "id": "mM2b4JLxX_98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = sorted({l for ex in ds for l in ex[\"labels_str\"]})\n",
        "label2id = {l:i for i,l in enumerate(label_list)}\n",
        "id2label = {i:l for l,i in label2id.items()}"
      ],
      "metadata": {
        "id": "EKY7ENyrvr7N"
      },
      "id": "EKY7ENyrvr7N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = ds.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "oPL5XMvetVxp"
      },
      "id": "oPL5XMvetVxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune Model"
      ],
      "metadata": {
        "id": "M664SUIhzTOF"
      },
      "id": "M664SUIhzTOF"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import set_seed\n",
        "\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
      ],
      "metadata": {
        "id": "GOSTAeNhyFr7"
      },
      "id": "GOSTAeNhyFr7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-{task}\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=training_epoch,\n",
        "    weight_decay=0.01,\n",
        "    seed=SEED,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "GeFrLxZqyFyT"
      },
      "id": "GeFrLxZqyFyT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch dataset\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "jpoN3XqtyF16"
      },
      "id": "jpoN3XqtyF16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GU9hfooeyF6H"
      },
      "id": "GU9hfooeyF6H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DxJDZqQyF9w"
      },
      "id": "0DxJDZqQyF9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SFerGjMyGBB"
      },
      "id": "7SFerGjMyGBB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "\n",
        "nb_path = \"/content/semantic_role_labeling/bert_finetuning.ipynb\"  # <- change this\n",
        "p = pathlib.Path(nb_path)\n",
        "\n",
        "nb = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "# Remove widget metadata that breaks nbconvert/GitHub rendering\n",
        "meta = nb.get(\"metadata\", {})\n",
        "if \"widgets\" in meta:\n",
        "    meta.pop(\"widgets\", None)\n",
        "    nb[\"metadata\"] = meta\n",
        "\n",
        "p.write_text(json.dumps(nb, ensure_ascii=False, indent=1), encoding=\"utf-8\")\n",
        "print(\"Cleaned:\", nb_path)"
      ],
      "metadata": {
        "id": "EzaTVRAlyGEK"
      },
      "id": "EzaTVRAlyGEK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[0]"
      ],
      "metadata": {
        "id": "3B-yPyUSwv6B"
      },
      "id": "3B-yPyUSwv6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenized_datasets[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "uhP07X0KxGJG"
      },
      "id": "uhP07X0KxGJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"instances\"][0])"
      ],
      "metadata": {
        "id": "mzomS_UrYQB8"
      },
      "id": "mzomS_UrYQB8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "id": "cRIr-Dqmxbkg"
      },
      "id": "cRIr-Dqmxbkg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "i0M84yqerZkN"
      },
      "id": "i0M84yqerZkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_list(dataset['instances'])"
      ],
      "metadata": {
        "id": "Ro37JJInraMo"
      },
      "id": "Ro37JJInraMo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "gxw2HXSNrsH7"
      },
      "id": "gxw2HXSNrsH7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "wV-aWy6Mr0Fk"
      },
      "id": "wV-aWy6Mr0Fk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}